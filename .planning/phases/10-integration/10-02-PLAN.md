---
phase: 10-integration
plan: 02
type: execute
wave: 2
depends_on: ["10-01"]
files_modified:
    - internal/orchestrator/run.go
    - internal/orchestrator/run_test.go
    - internal/orchestrator/integration_test.go
autonomous: true

must_haves:
    truths:
        - "Integration tests verify full flow: config -> state -> MCP -> process -> git -> logging"
        - "Tests use mock process spawner to avoid spawning real Claude"
        - "Tests verify signal handling (pass, fail, no-signal)"
        - "Tests verify state persistence across task boundaries"
        - "Tests verify git operations (branch creation, commits, resets)"
        - "Tests verify history recording (completed, failed, insights)"
        - "Unit tests cover edge cases (empty sprint, invalid config, context cancellation)"
    artifacts:
        - path: "internal/orchestrator/run.go"
          provides: "ProcessSpawner interface for testability"
          exports: ["ProcessSpawner"]
        - path: "internal/orchestrator/run_test.go"
          provides: "Unit tests for Run function edge cases"
          min_lines: 80
        - path: "internal/orchestrator/integration_test.go"
          provides: "Integration tests for end-to-end sprint execution"
          min_lines: 200
    key_links:
        - from: "internal/orchestrator/integration_test.go"
          to: "internal/orchestrator/run.go"
          via: "orchestrator.Run calls with mockSpawner"
          pattern: "Run\\(ctx.*Spawner:"
        - from: "internal/orchestrator/integration_test.go"
          to: "internal/mcp/server.go"
          via: "MCP client calls via simulateAgent using mockSpawner.lastPort"
          pattern: "simulateAgent.*lastPort"
---

<objective>
Create comprehensive integration and unit tests that verify the full sprint execution flow works correctly.

Purpose: Integration tests are CRITICAL to ensure all components (MCP server, process spawning, state machine, git operations, history logging) work together correctly. This is the primary deliverable ensuring the system is reliable.

Output: ProcessSpawner interface in `internal/orchestrator/run.go`, `internal/orchestrator/integration_test.go` with end-to-end tests, `internal/orchestrator/run_test.go` with unit tests for edge cases.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-integration/10-01-SUMMARY.md

@internal/orchestrator/run.go
@internal/orchestrator/handler.go
@internal/orchestrator/handler_test.go
@internal/mcp/server.go
@internal/mcp/server_test.go
@internal/testutil/tempdir.go
@internal/testutil/git.go
@internal/config/sprint.go
@internal/config/state.go
@internal/config/history.go
@internal/process/spawn.go
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add ProcessSpawner interface for testability</name>
  <files>internal/orchestrator/run.go</files>
  <action>
Add a ProcessSpawner interface to enable integration testing of the Run function. This MUST be done first because integration tests need to inject a mock spawner.

1.  **Define interface in run.go:**

    ```go
    // ProcessSpawner abstracts process creation for testing.
    type ProcessSpawner interface {
        Spawn(cfg process.SpawnConfig) (*process.SpawnResult, error)
    }

    // defaultSpawner uses the real process.SpawnClaude.
    type defaultSpawner struct{}

    func (defaultSpawner) Spawn(cfg process.SpawnConfig) (*process.SpawnResult, error) {
        return process.SpawnClaude(cfg)
    }
    ```

2.  **Add optional Spawner to RunConfig:**

    ```go
    type RunConfig struct {
        WorkDir    string
        SprintPath string
        Spawner    ProcessSpawner // Optional: defaults to real spawner
    }
    ```

3.  **Update Run to use Spawner:**

    ```go
    spawner := cfg.Spawner
    if spawner == nil {
        spawner = defaultSpawner{}
    }
    // In runTask, use spawner.Spawn instead of process.SpawnClaude
    ```

4.  **Update runTask signature** to accept ProcessSpawner and use it:

        ```go
        func runTask(ctx context.Context, cfg RunConfig, spawner ProcessSpawner, ...) (TaskResult, error) {
            // ...
            spawnResult, err := spawner.Spawn(process.SpawnConfig{...})
            // ...
        }
        ```

      </action>
      <verify>
      Compiles: `go build ./internal/orchestrator/...`
      Existing tests still pass: `go test ./internal/orchestrator/...`
      </verify>
      <done>
      ProcessSpawner interface exported. RunConfig accepts optional Spawner. runTask uses injected spawner. Existing behavior unchanged when Spawner is nil.
      </done>
    </task>

<task type="auto">
  <name>Task 2: Create integration tests using Run with mock spawner</name>
  <files>internal/orchestrator/integration_test.go</files>
  <action>
Create `internal/orchestrator/integration_test.go` with comprehensive integration tests that use the ProcessSpawner interface to test the full Run flow.

**Mock spawner that exposes MCPPort:**

```go
package orchestrator_test

import (
    "context"
    "os"
    "path/filepath"
    "strconv"
    "sync"
    "testing"
    "time"

    "github.com/mark3labs/mcp-go/client"
    "github.com/mark3labs/mcp-go/mcp"
    "github.com/sqve/kamaji/internal/config"
    "github.com/sqve/kamaji/internal/domain"
    "github.com/sqve/kamaji/internal/orchestrator"
    "github.com/sqve/kamaji/internal/process"
    "github.com/sqve/kamaji/internal/testutil"
    "gopkg.in/yaml.v3"
)

// mockSpawner captures SpawnConfig and provides test control.
// CRITICAL: lastPort stores MCPPort so tests can call simulateAgent.
type mockSpawner struct {
    mu       sync.Mutex
    lastPort int                 // MCPPort from last Spawn call - used by simulateAgent
    onSpawn  func(cfg process.SpawnConfig) // Called before returning
    done     chan struct{}       // Closed when "process" should exit
}

func newMockSpawner() *mockSpawner {
    return &mockSpawner{done: make(chan struct{})}
}

func (m *mockSpawner) Spawn(cfg process.SpawnConfig) (*process.SpawnResult, error) {
    m.mu.Lock()
    m.lastPort = cfg.MCPPort // Store port for simulateAgent
    if m.onSpawn != nil {
        m.onSpawn(cfg)
    }
    m.mu.Unlock()

    return &process.SpawnResult{
        Process:    &mockProcess{done: m.done},
        ConfigPath: filepath.Join(os.TempDir(), "test-mcp.json"),
    }, nil
}

func (m *mockSpawner) port() int {
    m.mu.Lock()
    defer m.mu.Unlock()
    return m.lastPort
}

func (m *mockSpawner) exit() {
    close(m.done)
}

// mockProcess implements process.Process interface for testing.
type mockProcess struct {
    done <-chan struct{}
}

func (p *mockProcess) Wait() error {
    <-p.done
    return nil
}

func (p *mockProcess) Kill() error {
    return nil
}

// writeTestSprint creates a kamaji.yaml file for testing
func writeTestSprint(t *testing.T, dir string, sprint *domain.Sprint) string {
    t.Helper()
    path := filepath.Join(dir, "kamaji.yaml")
    data, err := yaml.Marshal(sprint)
    if err != nil {
        t.Fatal(err)
    }
    if err := os.WriteFile(path, data, 0o600); err != nil {
        t.Fatal(err)
    }
    return path
}

// simulateAgent connects to MCP server at given port and sends a tool call.
// The port comes from mockSpawner.port() which captured it from SpawnConfig.MCPPort.
func simulateAgent(t *testing.T, port int, tool string, args map[string]any) {
    t.Helper()
    c, err := client.NewStreamableHttpClient("http://localhost:" + strconv.Itoa(port) + "/mcp")
    if err != nil {
        t.Fatalf("NewStreamableHttpClient: %v", err)
    }
    defer func() { _ = c.Close() }()

    ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
    defer cancel()

    _, err = c.Initialize(ctx, mcp.InitializeRequest{
        Params: mcp.InitializeParams{
            ProtocolVersion: "2024-11-05",
            ClientInfo:      mcp.Implementation{Name: "test-agent", Version: "1.0.0"},
        },
    })
    if err != nil {
        t.Fatalf("Initialize: %v", err)
    }

    _, err = c.CallTool(ctx, mcp.CallToolRequest{
        Params: mcp.CallToolParams{
            Name:      tool,
            Arguments: args,
        },
    })
    if err != nil {
        t.Fatalf("CallTool %s: %v", tool, err)
    }
}
```

**Integration tests (run with mock spawner):**

1. **TestIntegration_SingleTaskPass:**
    - Create temp dir with git repo (testutil.InitGitRepo)
    - Write sprint with 1 ticket, 1 task
    - Create mockSpawner, run orchestrator.Run with Spawner field set
    - In goroutine: wait for spawner.port() > 0, then simulateAgent with task_complete(pass, "Done")
    - After signal sent, call spawner.exit() to simulate process exit
    - Verify: result.Success, result.TasksRun==1, git commit exists, history.Completed has entry

2. **TestIntegration_SingleTaskFail:**
    - Same setup
    - Simulate agent: call task_complete(fail, "Tests failed"), then spawner.exit()
    - Verify: state.FailureCount==1, uncommitted files removed, history.FailedAttempts has entry

3. **TestIntegration_StuckAfterThreeFailures:**
    - Write sprint with 1 ticket, 1 task
    - Create mockSpawner that resets done channel between calls
    - Loop 3 times: wait for port, call task_complete(fail, "attempt N"), exit, wait for next spawn
    - Verify: result.Stuck==true, result.Success==false, state.FailureCount==3

4. **TestIntegration_MultiTaskSequence:**
    - Write sprint with 1 ticket, 3 tasks
    - Call task_complete(pass) + exit three times (resetting done channel between)
    - Verify: state advances correctly, 3 commits, all tasks in history.Completed

Pattern for running tests:

```go
func TestIntegration_SingleTaskPass(t *testing.T) {
    dir := t.TempDir()
    testutil.InitGitRepo(t, dir)

    sprint := &domain.Sprint{
        Name:       "test",
        BaseBranch: "main",
        Tickets: []domain.Ticket{{
            Name:   "TEST-1",
            Branch: "feat/test-1",
            Tasks:  []domain.Task{{Name: "Task 1", Prompt: "Do something"}},
        }},
    }
    sprintPath := writeTestSprint(t, dir, sprint)

    spawner := newMockSpawner()

    // Run orchestrator in goroutine
    var result *orchestrator.RunResult
    var runErr error
    runDone := make(chan struct{})
    go func() {
        result, runErr = orchestrator.Run(context.Background(), orchestrator.RunConfig{
            WorkDir:    dir,
            SprintPath: sprintPath,
            Spawner:    spawner,
        })
        close(runDone)
    }()

    // Wait for spawner to receive port, then simulate agent
    waitForPort(t, spawner, 5*time.Second)
    simulateAgent(t, spawner.port(), "task_complete", map[string]any{
        "signal":  "pass",
        "summary": "Done",
    })
    spawner.exit()

    // Wait for Run to complete
    select {
    case <-runDone:
    case <-time.After(10 * time.Second):
        t.Fatal("Run did not complete in time")
    }

    if runErr != nil {
        t.Fatalf("Run error: %v", runErr)
    }
    if !result.Success {
        t.Error("expected Success=true")
    }
    if result.TasksRun != 1 {
        t.Errorf("expected TasksRun=1, got %d", result.TasksRun)
    }
}

func waitForPort(t *testing.T, s *mockSpawner, timeout time.Duration) {
    t.Helper()
    deadline := time.Now().Add(timeout)
    for time.Now().Before(deadline) {
        if s.port() > 0 {
            return
        }
        time.Sleep(10 * time.Millisecond)
    }
    t.Fatal("spawner never received port")
}
```

  </action>
  <verify>
  Tests pass: `go test ./internal/orchestrator/... -v -run Integration`
  No race conditions: `go test ./internal/orchestrator/... -race -run Integration`
  </verify>
  <done>
  Integration tests verify: signal handling, state persistence, git operations, history recording, stuck detection. Tests use Run() with mockSpawner that exposes MCPPort for simulateAgent.
  </done>
</task>

<task type="auto">
  <name>Task 3: Create unit tests for Run function edge cases</name>
  <files>internal/orchestrator/run_test.go</files>
  <action>
Create `internal/orchestrator/run_test.go` with unit tests for edge cases that don't require full integration:

1. **TestRun_EmptySprint:**
    - Create temp dir with git repo
    - Write sprint with no tickets
    - Call Run
    - Verify: result.Success==true, result.TasksRun==0, no error

2. **TestRun_ConfigValidation_EmptyWorkDir:**
    - Call Run with empty WorkDir
    - Verify: returns error containing "WorkDir"

3. **TestRun_ConfigValidation_EmptySprintPath:**
    - Call Run with empty SprintPath
    - Verify: returns error containing "SprintPath"

4. **TestRun_InvalidSprintPath:**
    - Create temp dir (no kamaji.yaml)
    - Call Run
    - Verify: returns error about missing file

5. **TestRun_ContextCancellation:**
    - Create cancelled context
    - Call Run with mockSpawner
    - Verify: returns quickly (doesn't hang), error indicates cancellation

6. **TestRun_SprintWithEmptyTicket:**
    - Write sprint with 1 ticket that has 0 tasks
    - Call Run
    - Verify: result.Success==true, handled gracefully

```go
package orchestrator_test

import (
    "context"
    "path/filepath"
    "strings"
    "testing"

    "github.com/sqve/kamaji/internal/domain"
    "github.com/sqve/kamaji/internal/orchestrator"
    "github.com/sqve/kamaji/internal/testutil"
)

func TestRun_EmptySprint(t *testing.T) {
    dir := t.TempDir()
    testutil.InitGitRepo(t, dir)

    sprint := &domain.Sprint{
        Name:       "empty",
        BaseBranch: "main",
        Tickets:    []domain.Ticket{},
    }
    sprintPath := writeTestSprint(t, dir, sprint)

    result, err := orchestrator.Run(context.Background(), orchestrator.RunConfig{
        WorkDir:    dir,
        SprintPath: sprintPath,
    })
    if err != nil {
        t.Fatalf("Run() error = %v", err)
    }
    if !result.Success {
        t.Error("expected Success=true for empty sprint")
    }
    if result.TasksRun != 0 {
        t.Errorf("expected TasksRun=0, got %d", result.TasksRun)
    }
}

func TestRun_ConfigValidation_EmptyWorkDir(t *testing.T) {
    _, err := orchestrator.Run(context.Background(), orchestrator.RunConfig{
        SprintPath: "/some/path.yaml",
    })
    if err == nil {
        t.Fatal("expected error for empty WorkDir")
    }
    if !strings.Contains(err.Error(), "WorkDir") {
        t.Errorf("expected error about WorkDir, got: %v", err)
    }
}

func TestRun_ConfigValidation_EmptySprintPath(t *testing.T) {
    _, err := orchestrator.Run(context.Background(), orchestrator.RunConfig{
        WorkDir: "/some/dir",
    })
    if err == nil {
        t.Fatal("expected error for empty SprintPath")
    }
    if !strings.Contains(err.Error(), "SprintPath") {
        t.Errorf("expected error about SprintPath, got: %v", err)
    }
}

func TestRun_InvalidSprintPath(t *testing.T) {
    dir := t.TempDir()

    _, err := orchestrator.Run(context.Background(), orchestrator.RunConfig{
        WorkDir:    dir,
        SprintPath: filepath.Join(dir, "nonexistent.yaml"),
    })
    if err == nil {
        t.Fatal("expected error for missing sprint file")
    }
}
```

  </action>
  <verify>
  Tests pass: `go test ./internal/orchestrator/... -v -run TestRun`
  No race conditions: `go test ./internal/orchestrator/... -race`
  </verify>
  <done>
  Unit tests cover: empty sprint, config validation, invalid paths, context cancellation, empty tickets. All tests pass with no race conditions.
  </done>
</task>

</tasks>

<verification>
```bash
# Run all orchestrator tests
go test ./internal/orchestrator/... -v

# Check coverage

go test ./internal/orchestrator/... -cover

# Run with race detector

go test ./internal/orchestrator/... -race

# Run only integration tests

go test ./internal/orchestrator/... -v -run Integration

# Run full test suite

make test

# Run linter

make lint

```
</verification>

<success_criteria>
- ProcessSpawner interface exported from `internal/orchestrator/run.go`
- `internal/orchestrator/integration_test.go` exists with comprehensive tests using Run() + mockSpawner
- `internal/orchestrator/run_test.go` exists with edge case tests
- mockSpawner stores and exposes MCPPort for simulateAgent to connect
- Integration tests verify: MCP signals -> handler -> git -> history
- Tests cover: pass, fail, stuck, multi-task, insights
- All tests pass with race detector
- Coverage on orchestrator package is reasonable (>60%)
- Linter passes
- Full CI passes: `make ci`
</success_criteria>

<output>
After completion, create `.planning/phases/10-integration/10-02-SUMMARY.md`
</output>
```
